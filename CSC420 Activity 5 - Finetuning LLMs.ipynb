{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58bea6e7",
   "metadata": {},
   "source": [
    "# CSC420M Assignment 5: Finetuning Large Language Models\n",
    "Enrique Lejano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e735e4f",
   "metadata": {},
   "source": [
    "You will finetune 3 models for Named Entity Recognition (NER) on a TLUnified-NER dataset subset, identifying PERSON, ORGANIZATION, and LOCATION entities in Tagalog texts.  The 3 models to be finetuned are the following:\n",
    "\n",
    "- A Tagalog-pretrained model from jcblaise (local, e.g., jcblaise/distilbert-tagalog-base-uncased or jcblaise/roberta-tagalog-base)\n",
    "- SEA-LION (regional, e.g., aisingapore/sealion7b-instruct)\n",
    "- An open-source LLM, such as Gemma (multilingual, e.g., google/gemma-2b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8655c43",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbbf05",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342f3176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Insert pip install here\n",
    "%pip install --upgrade transformers datasets seqeval accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd478956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# No need to login because token already present in .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d6105",
   "metadata": {},
   "source": [
    "Select the three model checkpoints to be used.\n",
    "1. RoBERTa Tagalog\n",
    "2. Llama SEA-LION\n",
    "3. DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1c40b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tl_checkpoint = 'jcblaise/roberta-tagalog-base'\n",
    "sealion_checkpoint = 'aisingapore/Llama-SEA-LION-v3.5-8B-R'\n",
    "distilbert_checkpoint = 'distilbert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6716a0",
   "metadata": {},
   "source": [
    "## Load TLUnified-NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150f792e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "ner_tags",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "29a5e548-41f1-4dcb-961b-4c4bff5d1107",
       "rows": [
        [
         "0",
         "0",
         "['Sinabi', 'nito', 'na', 'lantad', 'ang', 'ginagawang', 'pagkuha', 'ng', 'mga', 'tauhan', 'ng', 'MMDA', 'sa', 'mahahalagang', 'gamit', 'ng', 'mga', 'vendors', 'na', 'hindi', 'pinipigil', 'ni', 'Fernando', '.']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]"
        ],
        [
         "1",
         "1",
         "['Kasama', 'rin', 'kasama', 'sa', 'ban', 'ang', 'mga', 'pribadong', 'indibidwal', 'na', 'nasa', 'emergency', 'cases', 'o', 'public', 'service', ';', 'mga', 'miyembro', 'ng', 'media', 'sa', 'TV', 'o', 'radio', 'networks', '\"', 'in', 'actual', 'performance', 'of', 'their', 'official', 'functions', '.', '\"', '-', 'GMANews', '.', 'TV']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
        ],
        [
         "2",
         "2",
         "['\"', 'Our', 'peace', 'advisers', 'informed', 'the', 'International', 'Monitoring', 'Team', 'and', 'the', 'combined', 'Coordinating', 'Committees', 'for', 'the', 'Cessation', 'of', 'Hostilities', '.', 'We', 'are', 'giving', 'those', 'individuals', 'that', 'forcibly', 'occupied', 'these', 'areas', '24', 'hours', 'to', 'vacate', '.', 'Otherwise', 'they', 'shall', 'be', 'forcibly', 'separated', 'from', 'the', 'area', '.', 'We', 'cannot', 'allow', 'these', 'things', 'to', 'happen', ',', '\"', 'ayon', 'kay', 'Puno', '.']",
         "[0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]"
        ],
        [
         "3",
         "3",
         "['Ayon', 'kay', 'Villavicencio', ',', 'kung', 'susuriin', 'ang', 'takbo', 'ng', 'ekonomya', 'sa', 'United', 'States', 'na', 'lubhang', 'nakakaapekto', 'sa', 'global', 'market', ',', 'tiyak', 'na', 'magkakaroon', 'umano', 'ng', 'panibagong', 'rollback', 'sa', 'presyo', 'ng', 'langis', '.']",
         "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
        ],
        [
         "4",
         "4",
         "['Sa', 'ulat', 'ng', 'dzBB', 'radio', 'nitong', 'Huwebes', ',', 'sinabi', 'umano', 'ni', 'Transportation', 'Undersecretary', 'Elena', 'Bautista', ',', 'pinuno', 'ng', 'Task', 'Force', 'Princess', 'na', 'mananatili', 'roon', 'ang', 'endosulfan', 'habang', 'inaayos', 'ang', 'pagbabalik', 'ng', 'kargamento', 'sa', 'Israel', '.']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Sinabi, nito, na, lantad, ang, ginagawang, pa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Kasama, rin, kasama, sa, ban, ang, mga, priba...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[\", Our, peace, advisers, informed, the, Inter...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 3, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Ayon, kay, Villavicencio, ,, kung, susuriin, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Sa, ulat, ng, dzBB, radio, nitong, Huwebes, ,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             tokens  \\\n",
       "0  0  [Sinabi, nito, na, lantad, ang, ginagawang, pa...   \n",
       "1  1  [Kasama, rin, kasama, sa, ban, ang, mga, priba...   \n",
       "2  2  [\", Our, peace, advisers, informed, the, Inter...   \n",
       "3  3  [Ayon, kay, Villavicencio, ,, kung, susuriin, ...   \n",
       "4  4  [Sa, ulat, ng, dzBB, radio, nitong, Huwebes, ,...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 3, 4, 4, ...  \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ljvmiranda921/tlunified-ner\", \"default\")\n",
    "df = DataFrame(ds['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fa16bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 6252\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 782\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 782\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a13da",
   "metadata": {},
   "source": [
    "Classification Label Equivalents: \n",
    "\n",
    "`0` : O \n",
    "\n",
    "`1` : `B-PER` -> Beginning of person name\n",
    "\n",
    "`2` : `I-PER` -> Inside person name\n",
    "\n",
    "`3` : `B-ORG` -> Beginning of organization\n",
    "\n",
    "`4` : `I-ORG` -> Inside an organization\n",
    "\n",
    "`5` : `B-LOC` -> Beginning of location\n",
    "\n",
    "`6` : `I-LOC` -> Inside of location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3bc4a6",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a712c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5e922",
   "metadata": {},
   "source": [
    "Create general tokenization and alignment code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a8cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, max_length=512):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True,\n",
    "        padding=True,  # Add padding\n",
    "        max_length=max_length,\n",
    "        return_tensors=None  # Don't convert to tensors yet\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens get -100\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First token of a word gets the label\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # Subsequent tokens of the same word get -100\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57573ac1",
   "metadata": {},
   "source": [
    "### Tokenization for `RoBERTa Tagalog` Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d87e976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f81633dcedc40b582938354c09fe7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenized and align labels first\n",
    "roberta_tl_tokenizer = AutoTokenizer.from_pretrained(roberta_tl_checkpoint, add_prefix_space=True)\n",
    "roberta_tokenized = partial(tokenize_and_align_labels, tokenizer=roberta_tl_tokenizer)\n",
    "\n",
    "# Map tokenization function to dataset\n",
    "roberta_tl_train = ds['train'].map(roberta_tokenized, batched=True, remove_columns=ds['train'].column_names)\n",
    "roberta_tl_val = ds['validation'].map(roberta_tokenized, batched=True, remove_columns=ds['validation'].column_names)\n",
    "roberta_tl_test = ds['test'].map(roberta_tokenized, batched=True, remove_columns=ds['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1abfc9",
   "metadata": {},
   "source": [
    "### Tokenization for `SEA-LION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9e00c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sealion_tokenizer = AutoTokenizer.from_pretrained(sealion_checkpoint, add_prefix_space=True)\n",
    "sealion_tokenized = partial(tokenize_and_align_labels, tokenizer=sealion_tokenizer)\n",
    "\n",
    "# Map tokenization function to dataset\n",
    "sealion_train = ds['train'].map(sealion_tokenized, batched=True, remove_columns=ds['train'].column_names)\n",
    "sealion_val = ds['validation'].map(sealion_tokenized, batched=True, remove_columns=ds['validation'].column_names)\n",
    "sealion_test = ds['test'].map(sealion_tokenized, batched=True, remove_columns=ds['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492769c",
   "metadata": {},
   "source": [
    "### Tokenization for `DistilBERT Multilingual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e323a267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc79d11c747140358e72b71cb90fa31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_checkpoint, add_prefix_space=True)\n",
    "distilbert_tokenized = partial(tokenize_and_align_labels, tokenizer=distilbert_tokenizer)\n",
    "\n",
    "distilbert_train = ds['train'].map(distilbert_tokenized, batched=True, remove_columns=ds['train'].column_names)\n",
    "distilbert_val = ds['validation'].map(distilbert_tokenized, batched=True, remove_columns=ds['validation'].column_names)\n",
    "distilbert_test = ds['test'].map(distilbert_tokenized, batched=True, remove_columns=ds['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92e684",
   "metadata": {},
   "source": [
    "## Finetuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f391b53",
   "metadata": {},
   "source": [
    "Define Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ed08261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "import evaluate\n",
    "\n",
    "# Load seqeval metric for NER evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Compute metrics function using seqeval\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(-1)\n",
    "    \n",
    "    # Remove ignored index (special tokens) and convert to label names\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Compute seqeval metrics\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee946aa",
   "metadata": {},
   "source": [
    "### Finetuning `RoBERTa Tagalog` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f1280",
   "metadata": {},
   "source": [
    "Load pretrained model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4f91ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at jcblaise/roberta-tagalog-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_tl_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    roberta_tl_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61deaea5",
   "metadata": {},
   "source": [
    "Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4e81a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-88e6be6c88b7>:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  roberta_tl_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Understand the training arguments and the concepts of epochs, batch size, learning rate, etc.\n",
    "\n",
    "# Data collator for token classification - this handles padding dynamically\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=roberta_tl_tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/roberta-tagalog-ner\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    warmup_steps=100,\n",
    "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_pin_memory=False,  # Set to False for M1\n",
    "    gradient_accumulation_steps=4,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "roberta_tl_trainer = Trainer(\n",
    "    model=roberta_tl_model,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tl_train,\n",
    "    eval_dataset=roberta_tl_val,\n",
    "    tokenizer=roberta_tl_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2efbe",
   "metadata": {},
   "source": [
    "Training `RoBERTa Tagalog` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a6d492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/1173 24:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>0.875829</td>\n",
       "      <td>0.901365</td>\n",
       "      <td>0.888413</td>\n",
       "      <td>0.983565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.058326</td>\n",
       "      <td>0.860885</td>\n",
       "      <td>0.917494</td>\n",
       "      <td>0.888288</td>\n",
       "      <td>0.982409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.056397</td>\n",
       "      <td>0.880695</td>\n",
       "      <td>0.911290</td>\n",
       "      <td>0.895732</td>\n",
       "      <td>0.983366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "roberta_tl_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "roberta_tl_trainer.save_model()\n",
    "roberta_tl_tokenizer.save_pretrained(\"./results/roberta-tagalog-ner\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fcf3f",
   "metadata": {},
   "source": [
    "### Finetuning `SEA-LION`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227614f",
   "metadata": {},
   "source": [
    "Load pretrained model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a8f2ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db9a6f73653419e962d052321d0aa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/840 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf09f42ad53e46069a52a59b4e392862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1706c1d79a4f049492edb15f676edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6a949b1c034e4f812c6a0b7cf82271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1f3830f18f439cb27c594ce7436336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ea6d38d2904e5f84c3f52e1600518b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5d61ec80e24e368196702bb41a3498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/34/a9/34a9b5653114d20f86708baab01d803830529d30f9285be6397756528167b434/b50a53788a3961497e4eedbef2a25faf255c2addcc9feaebdc0bc4b41109b802?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00003-of-00004.safetensors%3B+filename%3D%22model-00003-of-00004.safetensors%22%3B&Expires=1751363215&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzIxNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM0L2E5LzM0YTliNTY1MzExNGQyMGY4NjcwOGJhYWIwMWQ4MDM4MzA1MjlkMzBmOTI4NWJlNjM5Nzc1NjUyODE2N2I0MzQvYjUwYTUzNzg4YTM5NjE0OTdlNGVlZGJlZjJhMjVmYWYyNTVjMmFkZGNjOWZlYWViZGMwYmM0YjQxMTA5YjgwMj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=kSsECxMUKnlCZKCUBSzIlbedvbpT0p0KTVlov1-dZddTpznJ6qwmmOtOovx8SOLDtxwiqRdkVkCAYA9j07KshuHbD%7E4l0evjt0e6CiLO-Swxr4%7EMr37KxxAtLI6GvxWUpqRbnr-UaGKzpsvYTWzTqhB-FCM8mEG6qiYLx7NUa1AruDYiebBF4KwX80VlAybvGfdHlPXEhEioe9zinVv0dsmnY%7EqZpyPUZrVraEkvTs4kr8T5Zwanpbk7lroLXLEiP3h5iuSDnTZBH1bBP4JRvnpiJPpNJFxKzSE8uTeBA%7EwW3QGRNsoeRJGCJfQYeWN1amP687XeswrkR0fb0wkz1g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/34/a9/34a9b5653114d20f86708baab01d803830529d30f9285be6397756528167b434/99ffe2ae292309cfd47018de46ac6b66397d208b87420c7a89cfa5ec4b910fb0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00004.safetensors%3B+filename%3D%22model-00001-of-00004.safetensors%22%3B&Expires=1751363214&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzIxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM0L2E5LzM0YTliNTY1MzExNGQyMGY4NjcwOGJhYWIwMWQ4MDM4MzA1MjlkMzBmOTI4NWJlNjM5Nzc1NjUyODE2N2I0MzQvOTlmZmUyYWUyOTIzMDljZmQ0NzAxOGRlNDZhYzZiNjYzOTdkMjA4Yjg3NDIwYzdhODljZmE1ZWM0YjkxMGZiMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=enDomnRB4DhnSXAFHVF1f%7EcWA%7E13QosI9Qgb4QR9hGrMcDCwJpME-oRgwueVnxRL23ECkqZNh4Syif-0vl7Uyu2TwUtd9WI%7EcjY0J9V2UuPOzlTD5aBTr06VnUJ1AAqgWHS6EaH26jsvNe95wfH0k6xDwQ6Tc%7EOI7dXYsCee8FbL52rlC2I1A4pjHM5m0aBF0QJVBVOK4hKWNAHiHzCUw5aA5JwZ-4MHMOuWnyjyOlZLFAZufE7hS3qWTSt5JNWKlkBuOss-6X13DgpvyvprPq%7EaWVjhs1RvTDlziDpSPlrFrHF7wqWv7f1DpH%7ECc4LPSZwhAi4P1SEJVMwbg3UQsg__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/34/a9/34a9b5653114d20f86708baab01d803830529d30f9285be6397756528167b434/347f2c5bf1e6a4bd41ff30b4c36566bffdc53aac07d8b3b4059078477b5c90d3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00004.safetensors%3B+filename%3D%22model-00002-of-00004.safetensors%22%3B&Expires=1751363214&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzIxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM0L2E5LzM0YTliNTY1MzExNGQyMGY4NjcwOGJhYWIwMWQ4MDM4MzA1MjlkMzBmOTI4NWJlNjM5Nzc1NjUyODE2N2I0MzQvMzQ3ZjJjNWJmMWU2YTRiZDQxZmYzMGI0YzM2NTY2YmZmZGM1M2FhYzA3ZDhiM2I0MDU5MDc4NDc3YjVjOTBkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JT0lXBg3dHVM5yS0CJM27pzjQOAiIldIRQ61IX0XxbaP3kGiV3DLC8JlyCUkvJSjoBAgk2bynRPqhp1z2RGhTpZDIfeYVv6IXjQOzs6L9He4rfiEbGcu3iyUOMqemmeiD5CAmmMemLE-uN9Eg2umHlqE5k1AKq68X%7E4s3kDm%7EYAN5p5tMK%7EARsoW8%7EAO8z99iRzctoes5r8gCRzyggaUNSIFhw6kX0gnH21iRJSGacISfievzIWUHkwDEeK1lKG6ebvwzrzWSNCqA7J8ZjCloM%7E%7EdiWHff26X8AA%7EWk4i2RDvDnMMaXp7LcTLKAP84qbaFonBEmD4wYaDNEYhF%7E-OQ__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'cdn-lfs-us-1.hf.co\\', port=443): Max retries exceeded with url: /repos/34/a9/34a9b5653114d20f86708baab01d803830529d30f9285be6397756528167b434/99ffe2ae292309cfd47018de46ac6b66397d208b87420c7a89cfa5ec4b910fb0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00004.safetensors%3B+filename%3D%22model-00001-of-00004.safetensors%22%3B&Expires=1751363214&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzIxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM0L2E5LzM0YTliNTY1MzExNGQyMGY4NjcwOGJhYWIwMWQ4MDM4MzA1MjlkMzBmOTI4NWJlNjM5Nzc1NjUyODE2N2I0MzQvOTlmZmUyYWUyOTIzMDljZmQ0NzAxOGRlNDZhYzZiNjYzOTdkMjA4Yjg3NDIwYzdhODljZmE1ZWM0YjkxMGZiMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=enDomnRB4DhnSXAFHVF1f~cWA~13QosI9Qgb4QR9hGrMcDCwJpME-oRgwueVnxRL23ECkqZNh4Syif-0vl7Uyu2TwUtd9WI~cjY0J9V2UuPOzlTD5aBTr06VnUJ1AAqgWHS6EaH26jsvNe95wfH0k6xDwQ6Tc~OI7dXYsCee8FbL52rlC2I1A4pjHM5m0aBF0QJVBVOK4hKWNAHiHzCUw5aA5JwZ-4MHMOuWnyjyOlZLFAZufE7hS3qWTSt5JNWKlkBuOss-6X13DgpvyvprPq~aWVjhs1RvTDlziDpSPlrFrHF7wqWv7f1DpH~Cc4LPSZwhAi4P1SEJVMwbg3UQsg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3488dae50>: Failed to resolve \\'cdn-lfs-us-1.hf.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 620273bc-e66b-4b6c-839a-cd3e79514039)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:748\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:872\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m    873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:753\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BaseSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:494\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/models.py:826\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    200\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    202\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    203\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[1;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x3488dae50>: Failed to resolve 'cdn-lfs-us-1.hf.co' ([Errno 8] nodename nor servname provided, or not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Max retries exceeded with url: /repos/34/a9/34a9b5653114d20f86708baab01d803830529d30f9285be6397756528167b434/99ffe2ae292309cfd47018de46ac6b66397d208b87420c7a89cfa5ec4b910fb0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00004.safetensors%3B+filename%3D%22model-00001-of-00004.safetensors%22%3B&Expires=1751363214&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzIxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM0L2E5LzM0YTliNTY1MzExNGQyMGY4NjcwOGJhYWIwMWQ4MDM4MzA1MjlkMzBmOTI4NWJlNjM5Nzc1NjUyODE2N2I0MzQvOTlmZmUyYWUyOTIzMDljZmQ0NzAxOGRlNDZhYzZiNjYzOTdkMjA4Yjg3NDIwYzdhODljZmE1ZWM0YjkxMGZiMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=enDomnRB4DhnSXAFHVF1f~cWA~13QosI9Qgb4QR9hGrMcDCwJpME-oRgwueVnxRL23ECkqZNh4Syif-0vl7Uyu2TwUtd9WI~cjY0J9V2UuPOzlTD5aBTr06VnUJ1AAqgWHS6EaH26jsvNe95wfH0k6xDwQ6Tc~OI7dXYsCee8FbL52rlC2I1A4pjHM5m0aBF0QJVBVOK4hKWNAHiHzCUw5aA5JwZ-4MHMOuWnyjyOlZLFAZufE7hS3qWTSt5JNWKlkBuOss-6X13DgpvyvprPq~aWVjhs1RvTDlziDpSPlrFrHF7wqWv7f1DpH~Cc4LPSZwhAi4P1SEJVMwbg3UQsg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3488dae50>: Failed to resolve 'cdn-lfs-us-1.hf.co' ([Errno 8] nodename nor servname provided, or not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sealion_model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     sealion_checkpoint,\n\u001b[1;32m      3\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_list),\n\u001b[1;32m      4\u001b[0m     id2label\u001b[38;5;241m=\u001b[39mid2label,\n\u001b[1;32m      5\u001b[0m     label2id\u001b[38;5;241m=\u001b[39mlabel2id\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:4674\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4665\u001b[0m     gguf_file\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4667\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4668\u001b[0m ):\n\u001b[1;32m   4669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4671\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4672\u001b[0m     )\n\u001b[0;32m-> 4674\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m _get_resolved_checkpoint_files(\n\u001b[1;32m   4675\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[1;32m   4676\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   4677\u001b[0m     variant\u001b[38;5;241m=\u001b[39mvariant,\n\u001b[1;32m   4678\u001b[0m     gguf_file\u001b[38;5;241m=\u001b[39mgguf_file,\n\u001b[1;32m   4679\u001b[0m     from_tf\u001b[38;5;241m=\u001b[39mfrom_tf,\n\u001b[1;32m   4680\u001b[0m     from_flax\u001b[38;5;241m=\u001b[39mfrom_flax,\n\u001b[1;32m   4681\u001b[0m     use_safetensors\u001b[38;5;241m=\u001b[39muse_safetensors,\n\u001b[1;32m   4682\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   4683\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   4684\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   4685\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   4686\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   4687\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   4688\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   4689\u001b[0m     commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   4690\u001b[0m     is_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4691\u001b[0m     transformers_explicit_filename\u001b[38;5;241m=\u001b[39mtransformers_explicit_filename,\n\u001b[1;32m   4692\u001b[0m )\n\u001b[1;32m   4694\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4695\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:1295\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1293\u001b[0m sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m-> 1295\u001b[0m     checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   1296\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   1297\u001b[0m         resolved_archive_file,\n\u001b[1;32m   1298\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1299\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1300\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1301\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1302\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1303\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   1304\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1305\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   1306\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   1307\u001b[0m     )\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     checkpoint_files \u001b[38;5;241m=\u001b[39m [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:1110\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[0;32m-> 1110\u001b[0m cached_filenames \u001b[38;5;241m=\u001b[39m cached_files(\n\u001b[1;32m   1111\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1112\u001b[0m     shard_filenames,\n\u001b[1;32m   1113\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1114\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1115\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1116\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   1117\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1118\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1119\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   1120\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1121\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   1122\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39m_commit_hash,\n\u001b[1;32m   1123\u001b[0m )\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:557\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[0;32m--> 557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    559\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    560\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    561\u001b[0m ]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# If there are any missing file and the flag is active, raise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:485\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m         hf_hub_download(\n\u001b[1;32m    471\u001b[0m             path_or_repo_id,\n\u001b[1;32m    472\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    483\u001b[0m         )\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m         snapshot_download(\n\u001b[1;32m    486\u001b[0m             path_or_repo_id,\n\u001b[1;32m    487\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[1;32m    488\u001b[0m             repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    489\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    490\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    491\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    492\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    493\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    494\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    495\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    496\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    497\u001b[0m         )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py:297\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    295\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     thread_map(\n\u001b[1;32m    298\u001b[0m         _inner_hf_hub_download,\n\u001b[1;32m    299\u001b[0m         filtered_repo_files,\n\u001b[1;32m    300\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_repo_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    301\u001b[0m         max_workers\u001b[38;5;241m=\u001b[39mmax_workers,\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;66;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm_class \u001b[38;5;129;01mor\u001b[39;00m hf_tqdm,\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _executor_map(ThreadPoolExecutor, fn, \u001b[38;5;241m*\u001b[39miterables, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py:271\u001b[0m, in \u001b[0;36msnapshot_download.<locals>._inner_hf_hub_download\u001b[0;34m(repo_file)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_hf_hub_download\u001b[39m(repo_file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hf_hub_download(\n\u001b[1;32m    272\u001b[0m         repo_id,\n\u001b[1;32m    273\u001b[0m         filename\u001b[38;5;241m=\u001b[39mrepo_file,\n\u001b[1;32m    274\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    275\u001b[0m         revision\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    276\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m    277\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    278\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m    279\u001b[0m         local_dir_use_symlinks\u001b[38;5;241m=\u001b[39mlocal_dir_use_symlinks,\n\u001b[1;32m    280\u001b[0m         library_name\u001b[38;5;241m=\u001b[39mlibrary_name,\n\u001b[1;32m    281\u001b[0m         library_version\u001b[38;5;241m=\u001b[39mlibrary_version,\n\u001b[1;32m    282\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    283\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    284\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m    285\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    286\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    287\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    288\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    289\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1009\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1013\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1014\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1015\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1018\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1019\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[1;32m   1020\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1021\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1024\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1025\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1159\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1157\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1159\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1160\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1161\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[1;32m   1162\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[1;32m   1163\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1164\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1165\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1166\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1167\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1168\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[1;32m   1169\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[1;32m   1170\u001b[0m     )\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1172\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1717\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1718\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1719\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1720\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1721\u001b[0m             )\n\u001b[0;32m-> 1723\u001b[0m         http_get(\n\u001b[1;32m   1724\u001b[0m             url_to_download,\n\u001b[1;32m   1725\u001b[0m             f,\n\u001b[1;32m   1726\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1727\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[1;32m   1728\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1729\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1730\u001b[0m         )\n\u001b[1;32m   1732\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1733\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:511\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    509\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m         reset_sessions()  \u001b[38;5;66;03m# In case of SSLError it's best to reset the shared requests.Session objects\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m http_get(\n\u001b[1;32m    512\u001b[0m             url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    513\u001b[0m             temp_file\u001b[38;5;241m=\u001b[39mtemp_file,\n\u001b[1;32m    514\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    515\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mnew_resume_size,\n\u001b[1;32m    516\u001b[0m             headers\u001b[38;5;241m=\u001b[39minitial_headers,\n\u001b[1;32m    517\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m    518\u001b[0m             _nb_retries\u001b[38;5;241m=\u001b[39m_nb_retries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    519\u001b[0m             _tqdm_bar\u001b[38;5;241m=\u001b[39m_tqdm_bar,\n\u001b[1;32m    520\u001b[0m         )\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    524\u001b[0m         consistency_error_message\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    525\u001b[0m             actual_size\u001b[38;5;241m=\u001b[39mtemp_file\u001b[38;5;241m.\u001b[39mtell(),\n\u001b[1;32m    526\u001b[0m         )\n\u001b[1;32m    527\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:420\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file is too large to be downloaded using the regular download method. Use `hf_transfer` or `hf_xet` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Try `pip install hf_transfer` or `pip install hf_xet`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         )\n\u001b[0;32m--> 420\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    421\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39murl, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_DOWNLOAD_TIMEOUT\n\u001b[1;32m    422\u001b[0m )\n\u001b[1;32m    424\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    425\u001b[0m content_length \u001b[38;5;241m=\u001b[39m _get_file_length_from_http_response(r)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[1;32m    310\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'cdn-lfs-us-1.hf.co\\', port=443): Max retries exceeded with url: /repos/34/a9/34a9b5653114d20f86708baab01d803830529d30f9285be6397756528167b434/99ffe2ae292309cfd47018de46ac6b66397d208b87420c7a89cfa5ec4b910fb0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00004.safetensors%3B+filename%3D%22model-00001-of-00004.safetensors%22%3B&Expires=1751363214&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTM2MzIxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM0L2E5LzM0YTliNTY1MzExNGQyMGY4NjcwOGJhYWIwMWQ4MDM4MzA1MjlkMzBmOTI4NWJlNjM5Nzc1NjUyODE2N2I0MzQvOTlmZmUyYWUyOTIzMDljZmQ0NzAxOGRlNDZhYzZiNjYzOTdkMjA4Yjg3NDIwYzdhODljZmE1ZWM0YjkxMGZiMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=enDomnRB4DhnSXAFHVF1f~cWA~13QosI9Qgb4QR9hGrMcDCwJpME-oRgwueVnxRL23ECkqZNh4Syif-0vl7Uyu2TwUtd9WI~cjY0J9V2UuPOzlTD5aBTr06VnUJ1AAqgWHS6EaH26jsvNe95wfH0k6xDwQ6Tc~OI7dXYsCee8FbL52rlC2I1A4pjHM5m0aBF0QJVBVOK4hKWNAHiHzCUw5aA5JwZ-4MHMOuWnyjyOlZLFAZufE7hS3qWTSt5JNWKlkBuOss-6X13DgpvyvprPq~aWVjhs1RvTDlziDpSPlrFrHF7wqWv7f1DpH~Cc4LPSZwhAi4P1SEJVMwbg3UQsg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3488dae50>: Failed to resolve \\'cdn-lfs-us-1.hf.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 620273bc-e66b-4b6c-839a-cd3e79514039)')"
     ]
    }
   ],
   "source": [
    "sealion_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    sealion_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fc1f3",
   "metadata": {},
   "source": [
    "Define data collator and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for token classification - this handles padding dynamically\n",
    "sealion_data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=sealion_tokenizer,\n",
    "    padding=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "   \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/sealion-3.5-ner\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    warmup_steps=100,\n",
    "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_pin_memory=False,  # Set to False for M1\n",
    "    gradient_accumulation_steps=4,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "sealion_trainer = Trainer(\n",
    "    model=sealion_model,\n",
    "    args=training_args,\n",
    "    train_dataset=sealion_train['train'],\n",
    "    eval_dataset=sealion_val['validation'],\n",
    "    tokenizer=sealion_tokenizer,\n",
    "    data_collator=sealion_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fbb7e",
   "metadata": {},
   "source": [
    "Start training for SEALION model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa559b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sealion_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sealion_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sealion_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "sealion_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "sealion_trainer.save_model()\n",
    "sealion_tokenizer.save_pretrained(\"./results/sealion-3.5-ner\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3663d2",
   "metadata": {},
   "source": [
    "### Finetuning `DistilBERT Multilingual Cased`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1462d73",
   "metadata": {},
   "source": [
    "Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77f7c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    distilbert_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa86573",
   "metadata": {},
   "source": [
    "Define data collator and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3d64c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-55159d56687d>:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  distilbert_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Data collator for token classification - this handles padding dynamically\n",
    "distilbert_data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=distilbert_tokenizer,\n",
    "    padding=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "   \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/distilbert-multilingual-cased-ner\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    warmup_steps=100,\n",
    "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_pin_memory=False,  # Set to False for M1\n",
    "    gradient_accumulation_steps=4,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "distilbert_trainer = Trainer(\n",
    "    model=distilbert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=distilbert_train,\n",
    "    eval_dataset=distilbert_val,\n",
    "    tokenizer=distilbert_tokenizer,\n",
    "    data_collator=distilbert_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f53f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/1173 11:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.073474</td>\n",
       "      <td>0.833525</td>\n",
       "      <td>0.897643</td>\n",
       "      <td>0.864397</td>\n",
       "      <td>0.978858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.059822</td>\n",
       "      <td>0.876588</td>\n",
       "      <td>0.898883</td>\n",
       "      <td>0.887596</td>\n",
       "      <td>0.982568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>0.881098</td>\n",
       "      <td>0.896402</td>\n",
       "      <td>0.888684</td>\n",
       "      <td>0.983087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "distilbert_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "distilbert_trainer.save_model()\n",
    "distilbert_tokenizer.save_pretrained(\"./results/distilbert-multilingual-cased-ner\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ddbb7b",
   "metadata": {},
   "source": [
    "## Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d19fca",
   "metadata": {},
   "source": [
    "Create helper function to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4ad3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_finetuned_model(model_path, trainer, test_dataset, label_list, id2label, label2id):\n",
    "    print(f'Loading finetuned model from: {model_path}')\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    finetuned_model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)\n",
    "\n",
    "    # Use MPS if possible, otherwise use CUDA or CPU depending on device compatibility\n",
    "    if torch.backends.mps.is_available():\n",
    "        finetuned_model = finetuned_model.to('mps')\n",
    "    elif torch.cuda.is_available():\n",
    "        finetuned_model = finetuned_model.to('cuda')\n",
    "    else:\n",
    "        finetuned_model = finetuned_model.to('cpu')\n",
    "\n",
    "    # Update trainer\n",
    "    trainer.model = finetuned_model\n",
    "    trainer.processing_class = tokenizer\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    prediction_output = trainer.predict(test_dataset)\n",
    "\n",
    "    # Extract predictions and labels correctly\n",
    "    predictions = prediction_output.predictions.argmax(-1)\n",
    "    labels = prediction_output.label_ids\n",
    "\n",
    "    # Convert IDs to label names, ignoring -100 (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p, l in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    print(\"Predictions complete!\")\n",
    "    \n",
    "    return true_predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9abc84",
   "metadata": {},
   "source": [
    "Create helper function to display model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62cb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def print_predictions(true_predictions, true_labels, model_checkpoint):\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    total_results = 0\n",
    "\n",
    "    # Extract entity-level results\n",
    "    entity_data = []\n",
    "    for entity in ['PER', 'ORG', 'LOC']:\n",
    "        if entity in results:\n",
    "            entity_data.append({\n",
    "                'Entity': entity,\n",
    "                'Precision': f\"{results[entity]['precision']:.4f}\",\n",
    "                'Recall': f\"{results[entity]['recall']:.4f}\",\n",
    "                'F1-Score': f\"{results[entity]['f1']:.4f}\",\n",
    "                'Support': results[entity]['number']\n",
    "            })\n",
    "            total_results += results[entity]['number']\n",
    "\n",
    "    # Add overall results\n",
    "    entity_data.append({\n",
    "        'Entity': 'OVERALL',\n",
    "        'Precision': f\"{results['overall_precision']:.4f}\",\n",
    "        'Recall': f\"{results['overall_recall']:.4f}\",\n",
    "        'F1-Score': f\"{results['overall_f1']:.4f}\",\n",
    "        'Support': f\"{total_results}\"\n",
    "    })         \n",
    "\n",
    "    # Create and display table\n",
    "    df = pd.DataFrame(entity_data)\n",
    "    print(f\"Test Results for Model {model_checkpoint}:\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nOverall Accuracy: {results['overall_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205b822",
   "metadata": {},
   "source": [
    "### `RoBERTa Tagalog` Prediction on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8177c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model from: ./results/roberta-tagalog-ner\n",
      "Making predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete!\n"
     ]
    }
   ],
   "source": [
    "true_predictions, true_labels = evaluate_finetuned_model(\n",
    "    model_path='./results/roberta-tagalog-ner',\n",
    "    trainer=roberta_tl_trainer,\n",
    "    test_dataset=roberta_tl_test,\n",
    "    label_list=label_list,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d78e2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for Model RoBERTa Tagalog:\n",
      " Entity Precision Recall F1-Score Support\n",
      "    PER    0.9246 0.9268   0.9257     833\n",
      "    ORG    0.7694 0.8457   0.8058     363\n",
      "    LOC    0.8377 0.9164   0.8753     383\n",
      "OVERALL    0.8651 0.9056   0.8849    1579\n",
      "\n",
      "Overall Accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "print_predictions(true_predictions, true_labels, model_checkpoint='RoBERTa Tagalog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa4b50",
   "metadata": {},
   "source": [
    "### `DistilBERT Multilingual Cased` Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8524d9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model from: ./results/distilbert-multilingual-cased-ner\n",
      "Making predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete!\n"
     ]
    }
   ],
   "source": [
    "true_predictions, true_labels = evaluate_finetuned_model(\n",
    "    model_path='./results/distilbert-multilingual-cased-ner',\n",
    "    trainer=distilbert_trainer,\n",
    "    test_dataset=distilbert_test,\n",
    "    label_list=label_list,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d1c3684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for Model DistilBERT Multilingual Cased:\n",
      " Entity Precision Recall F1-Score Support\n",
      "    PER    0.9262 0.9196   0.9229     833\n",
      "    ORG    0.7279 0.8402   0.7801     363\n",
      "    LOC    0.8883 0.9138   0.9009     383\n",
      "OVERALL    0.8665 0.8999   0.8829    1579\n",
      "\n",
      "Overall Accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "print_predictions(true_predictions, true_labels, model_checkpoint='DistilBERT Multilingual Cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9e1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
