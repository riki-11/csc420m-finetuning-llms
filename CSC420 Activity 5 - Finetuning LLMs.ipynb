{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58bea6e7",
   "metadata": {},
   "source": [
    "# CSC420M Assignment 5: Finetuning Large Language Models\n",
    "Enrique Lejano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e735e4f",
   "metadata": {},
   "source": [
    "You will finetune 3 models for Named Entity Recognition (NER) on a TLUnified-NER dataset subset, identifying PERSON, ORGANIZATION, and LOCATION entities in Tagalog texts.  The 3 models to be finetuned are the following:\n",
    "\n",
    "- A Tagalog-pretrained model from jcblaise (local, e.g., jcblaise/distilbert-tagalog-base-uncased or jcblaise/roberta-tagalog-base)\n",
    "- SEA-LION (regional, e.g., aisingapore/sealion7b-instruct)\n",
    "- An open-source LLM, such as Gemma (multilingual, e.g., google/gemma-2b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8655c43",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbbf05",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342f3176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Insert pip install here\n",
    "%pip install --upgrade transformers datasets seqeval accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c484e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (2.7.1+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (0.22.1+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (2.7.1+cu128)\n",
      "Requirement already satisfied: filelock in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\enrique\\anaconda3\\envs\\anaconda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Only for my windows machine.\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212da9da",
   "metadata": {},
   "source": [
    "Use CUDA with GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ed5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd478956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# No need to login because token already present in .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d6105",
   "metadata": {},
   "source": [
    "Select the three model checkpoints to be used.\n",
    "1. RoBERTa Tagalog\n",
    "2. Llama SEA-LION\n",
    "3. DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c40b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tl_checkpoint = 'jcblaise/roberta-tagalog-base'\n",
    "sealion_checkpoint = 'aisingapore/Llama-SEA-LION-v3.5-8B-R'\n",
    "distilbert_checkpoint = 'distilbert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6716a0",
   "metadata": {},
   "source": [
    "## Load TLUnified-NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150f792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enrique\\anaconda3\\envs\\anaconda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Sinabi, nito, na, lantad, ang, ginagawang, pa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Kasama, rin, kasama, sa, ban, ang, mga, priba...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[\", Our, peace, advisers, informed, the, Inter...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 3, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Ayon, kay, Villavicencio, ,, kung, susuriin, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Sa, ulat, ng, dzBB, radio, nitong, Huwebes, ,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             tokens  \\\n",
       "0  0  [Sinabi, nito, na, lantad, ang, ginagawang, pa...   \n",
       "1  1  [Kasama, rin, kasama, sa, ban, ang, mga, priba...   \n",
       "2  2  [\", Our, peace, advisers, informed, the, Inter...   \n",
       "3  3  [Ayon, kay, Villavicencio, ,, kung, susuriin, ...   \n",
       "4  4  [Sa, ulat, ng, dzBB, radio, nitong, Huwebes, ,...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 3, 4, 4, ...  \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ljvmiranda921/tlunified-ner\", \"default\")\n",
    "df = DataFrame(ds['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a13da",
   "metadata": {},
   "source": [
    "Classification Label Equivalents: \n",
    "\n",
    "`0` : O \n",
    "\n",
    "`1` : `B-PER` -> Beginning of person name\n",
    "\n",
    "`2` : `I-PER` -> Inside person name\n",
    "\n",
    "`3` : `B-ORG` -> Beginning of organization\n",
    "\n",
    "`4` : `I-ORG` -> Inside an organization\n",
    "\n",
    "`5` : `B-LOC` -> Beginning of location\n",
    "\n",
    "`6` : `I-LOC` -> Inside of location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3bc4a6",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a712c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5e922",
   "metadata": {},
   "source": [
    "Create general tokenization and alignment code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a8cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, max_length=512):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True,\n",
    "        padding=True,  # Add padding\n",
    "        max_length=max_length,\n",
    "        return_tensors=None  # Don't convert to tensors yet\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens get -100\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First token of a word gets the label\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # Subsequent tokens of the same word get -100\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57573ac1",
   "metadata": {},
   "source": [
    "### Tokenization for `RoBERTa Tagalog` Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d87e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized and align labels first\n",
    "roberta_tl_tokenizer = AutoTokenizer.from_pretrained(roberta_tl_checkpoint, add_prefix_space=True)\n",
    "roberta_tokenized = partial(tokenize_and_align_labels, tokenizer=roberta_tl_tokenizer)\n",
    "\n",
    "# Map tokenization function to dataset\n",
    "roberta_tl_train = ds['train'].map(roberta_tokenized, batched=True, remove_columns=ds['train'].column_names)\n",
    "roberta_tl_val = ds['validation'].map(roberta_tokenized, batched=True, remove_columns=ds['validation'].column_names)\n",
    "roberta_tl_test = ds['test'].map(roberta_tokenized, batched=True, remove_columns=ds['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1abfc9",
   "metadata": {},
   "source": [
    "### Tokenization for `SEA-LION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e00c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sealion_tokenizer = AutoTokenizer.from_pretrained(sealion_checkpoint, add_prefix_space=True)\n",
    "sealion_tokenized = partial(tokenize_and_align_labels, tokenizer=sealion_tokenizer)\n",
    "\n",
    "# Map tokenization function to dataset\n",
    "sealion_train = ds['train'].map(sealion_tokenized, batched=True, remove_columns=ds['train'].column_names)\n",
    "sealion_val = ds['validation'].map(sealion_tokenized, batched=True, remove_columns=ds['validation'].column_names)\n",
    "sealion_test = ds['test'].map(sealion_tokenized, batched=True, remove_columns=ds['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492769c",
   "metadata": {},
   "source": [
    "### Tokenization for `DistilBERT Multilingual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e323a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_checkpoint, add_prefix_space=True)\n",
    "distilbert_tokenized = partial(tokenize_and_align_labels, tokenizer=distilbert_tokenizer)\n",
    "\n",
    "distilbert_train = ds['train'].map(distilbert_tokenized, batched=True, remove_columns=ds['train'].column_names)\n",
    "distilbert_val = ds['validation'].map(distilbert_tokenized, batched=True, remove_columns=ds['validation'].column_names)\n",
    "distilbert_test = ds['test'].map(distilbert_tokenized, batched=True, remove_columns=ds['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92e684",
   "metadata": {},
   "source": [
    "## Finetuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f391b53",
   "metadata": {},
   "source": [
    "Define Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed08261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "import evaluate\n",
    "\n",
    "# Load seqeval metric for NER evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Compute metrics function using seqeval\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(-1)\n",
    "    \n",
    "    # Remove ignored index (special tokens) and convert to label names\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Compute seqeval metrics\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee946aa",
   "metadata": {},
   "source": [
    "### Finetuning `RoBERTa Tagalog` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f1280",
   "metadata": {},
   "source": [
    "Load pretrained model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f91ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at jcblaise/roberta-tagalog-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_tl_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    roberta_tl_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    roberta_tl_model = roberta_tl_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61deaea5",
   "metadata": {},
   "source": [
    "Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4e81a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enrique\\AppData\\Local\\Temp\\ipykernel_28652\\94787007.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  roberta_tl_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Understand the training arguments and the concepts of epochs, batch size, learning rate, etc.\n",
    "\n",
    "# Data collator for token classification - this handles padding dynamically\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=roberta_tl_tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/roberta-tagalog-ner\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    warmup_steps=100,\n",
    "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_pin_memory=False,  # Set to False for M1\n",
    "    gradient_accumulation_steps=4,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "roberta_tl_trainer = Trainer(\n",
    "    model=roberta_tl_model,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tl_train,\n",
    "    eval_dataset=roberta_tl_val,\n",
    "    tokenizer=roberta_tl_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2efbe",
   "metadata": {},
   "source": [
    "Training `RoBERTa Tagalog` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a6d492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/1173 09:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.057932</td>\n",
       "      <td>0.873722</td>\n",
       "      <td>0.901365</td>\n",
       "      <td>0.887328</td>\n",
       "      <td>0.982967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.058897</td>\n",
       "      <td>0.860737</td>\n",
       "      <td>0.912531</td>\n",
       "      <td>0.885878</td>\n",
       "      <td>0.981930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.057458</td>\n",
       "      <td>0.880767</td>\n",
       "      <td>0.911911</td>\n",
       "      <td>0.896068</td>\n",
       "      <td>0.983446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "roberta_tl_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "roberta_tl_trainer.save_model()\n",
    "roberta_tl_tokenizer.save_pretrained(\"./results/roberta-tagalog-ner\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3663d2",
   "metadata": {},
   "source": [
    "### Finetuning `DistilBERT Multilingual Cased`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77f7c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    distilbert_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    distilbert_model = distilbert_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77f7c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enrique\\AppData\\Local\\Temp\\ipykernel_28652\\2063078941.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  distilbert_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Data collator for token classification - this handles padding dynamically\n",
    "distilbert_data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=distilbert_tokenizer,\n",
    "    padding=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "   \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/distilbert-multilingual-cased-ner\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    warmup_steps=100,\n",
    "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_pin_memory=False,  # Set to False for M1\n",
    "    gradient_accumulation_steps=4,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "distilbert_trainer = Trainer(\n",
    "    model=distilbert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=distilbert_train,\n",
    "    eval_dataset=distilbert_val,\n",
    "    tokenizer=distilbert_tokenizer,\n",
    "    data_collator=distilbert_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f53f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/1173 05:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.069936</td>\n",
       "      <td>0.844484</td>\n",
       "      <td>0.892680</td>\n",
       "      <td>0.867913</td>\n",
       "      <td>0.980055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.061374</td>\n",
       "      <td>0.871437</td>\n",
       "      <td>0.891439</td>\n",
       "      <td>0.881325</td>\n",
       "      <td>0.982010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.060278</td>\n",
       "      <td>0.870138</td>\n",
       "      <td>0.901985</td>\n",
       "      <td>0.885775</td>\n",
       "      <td>0.982688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "distilbert_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "distilbert_trainer.save_model()\n",
    "distilbert_tokenizer.save_pretrained(\"./results/distilbert-multilingual-cased-ner\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fcf3f",
   "metadata": {},
   "source": [
    "### Finetuning `SEA-LION`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d492f",
   "metadata": {},
   "source": [
    "Load pretrained model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f2ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enrique\\anaconda3\\envs\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Enrique\\.cache\\huggingface\\hub\\models--aisingapore--Llama-SEA-LION-v3.5-8B-R. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "sealion_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    sealion_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    sealion_model = sealion_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fc1f3",
   "metadata": {},
   "source": [
    "Define data collator and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for token classification - this handles padding dynamically\n",
    "sealion_data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=sealion_tokenizer,\n",
    "    padding=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "   \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/sealion-3.5-ner\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    warmup_steps=100,\n",
    "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_pin_memory=False,  # Set to False for M1\n",
    "    gradient_accumulation_steps=4,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "sealion_trainer = Trainer(\n",
    "    model=sealion_model,\n",
    "    args=training_args,\n",
    "    train_dataset=sealion_train['train'],\n",
    "    eval_dataset=sealion_val['validation'],\n",
    "    tokenizer=sealion_tokenizer,\n",
    "    data_collator=sealion_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fbb7e",
   "metadata": {},
   "source": [
    "Start training for SEALION model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa559b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sealion_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sealion_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sealion_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "sealion_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "sealion_trainer.save_model()\n",
    "sealion_tokenizer.save_pretrained(\"./results/sealion-3.5-ner\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1462d73",
   "metadata": {},
   "source": [
    "Load pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa86573",
   "metadata": {},
   "source": [
    "Define data collator and training arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d64c25",
   "metadata": {},
   "source": [
    "## Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53f3c5",
   "metadata": {},
   "source": [
    "Create helper function to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ad3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_finetuned_model(model_path, trainer, test_dataset, label_list, id2label, label2id):\n",
    "    print(f'Loading finetuned model from: {model_path}')\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    finetuned_model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)\n",
    "\n",
    "    # TODO: switch top to cuda here.\n",
    "    # Use MPS if possible, otherwise use CUDA or CPU depending on device compatibility\n",
    "    if torch.cuda.is_available():\n",
    "        finetuned_model = finetuned_model.to('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        finetuned_model = finetuned_model.to('mps')\n",
    "    else:\n",
    "        finetuned_model = finetuned_model.to('cpu')\n",
    "\n",
    "    # Update trainer\n",
    "    trainer.model = finetuned_model\n",
    "    trainer.processing_class = tokenizer\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    prediction_output = trainer.predict(test_dataset)\n",
    "\n",
    "    # Extract predictions and labels correctly\n",
    "    predictions = prediction_output.predictions.argmax(-1)\n",
    "    labels = prediction_output.label_ids\n",
    "\n",
    "    # Convert IDs to label names, ignoring -100 (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p, l in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    print(\"Predictions complete!\")\n",
    "    \n",
    "    return true_predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9abc84",
   "metadata": {},
   "source": [
    "Create helper function to display model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ad3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def print_predictions(true_predictions, true_labels, model_checkpoint):\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    total_results = 0\n",
    "\n",
    "    # Extract entity-level results\n",
    "    entity_data = []\n",
    "    for entity in ['PER', 'ORG', 'LOC']:\n",
    "        if entity in results:\n",
    "            entity_data.append({\n",
    "                'Entity': entity,\n",
    "                'Precision': f\"{results[entity]['precision']:.4f}\",\n",
    "                'Recall': f\"{results[entity]['recall']:.4f}\",\n",
    "                'F1-Score': f\"{results[entity]['f1']:.4f}\",\n",
    "                'Support': results[entity]['number']\n",
    "            })\n",
    "            total_results += results[entity]['number']\n",
    "\n",
    "    # Add overall results\n",
    "    entity_data.append({\n",
    "        'Entity': 'OVERALL',\n",
    "        'Precision': f\"{results['overall_precision']:.4f}\",\n",
    "        'Recall': f\"{results['overall_recall']:.4f}\",\n",
    "        'F1-Score': f\"{results['overall_f1']:.4f}\",\n",
    "        'Support': f\"{total_results}\"\n",
    "    })         \n",
    "\n",
    "    # Create and display table\n",
    "    df = pd.DataFrame(entity_data)\n",
    "    print(f\"Test Results for Model {model_checkpoint}:\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nOverall Accuracy: {results['overall_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205b822",
   "metadata": {},
   "source": [
    "### `RoBERTa Tagalog` Prediction on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d62cb0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model from: ./results/roberta-tagalog-ner\n",
      "Making predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete!\n"
     ]
    }
   ],
   "source": [
    "true_predictions, true_labels = evaluate_finetuned_model(\n",
    "    model_path='./results/roberta-tagalog-ner',\n",
    "    trainer=roberta_tl_trainer,\n",
    "    test_dataset=roberta_tl_test,\n",
    "    label_list=label_list,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d78e2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for Model RoBERTa Tagalog:\n",
      " Entity Precision Recall F1-Score Support\n",
      "    PER    0.9314 0.9292   0.9303     833\n",
      "    ORG    0.7625 0.8402   0.7995     363\n",
      "    LOC    0.8502 0.9191   0.8833     383\n",
      "OVERALL    0.8699 0.9063   0.8877    1579\n",
      "\n",
      "Overall Accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "print_predictions(true_predictions, true_labels, model_checkpoint='RoBERTa Tagalog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8177c82",
   "metadata": {},
   "source": [
    "### `DistilBERT Multilingual Cased` Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d78e2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model from: ./results/distilbert-multilingual-cased-ner\n",
      "Making predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete!\n"
     ]
    }
   ],
   "source": [
    "true_predictions, true_labels = evaluate_finetuned_model(\n",
    "    model_path='./results/distilbert-multilingual-cased-ner',\n",
    "    trainer=distilbert_trainer,\n",
    "    test_dataset=distilbert_test,\n",
    "    label_list=label_list,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1c3684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for Model DistilBERT Multilingual Cased:\n",
      " Entity Precision Recall F1-Score Support\n",
      "    PER    0.9158 0.9136   0.9147     833\n",
      "    ORG    0.7214 0.8347   0.7739     363\n",
      "    LOC    0.8700 0.9086   0.8889     383\n",
      "OVERALL    0.8552 0.8942   0.8743    1579\n",
      "\n",
      "Overall Accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "print_predictions(true_predictions, true_labels, model_checkpoint='DistilBERT Multilingual Cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8524d9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
